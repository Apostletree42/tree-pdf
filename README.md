# tree-pdf

# PDF Q&A Application

A full-stack RAG (Retrieval Augmented Generation) application that allows users to upload PDF documents and ask intelligent questions about their content using AI.

## ğŸš€ Features

- **PDF Upload & Processing**: Upload PDF documents with automatic text extraction
- **Intelligent Q&A**: Ask questions and get AI-powered answers based on document content
- **Vector Search**: Uses FAISS for semantic similarity search
- **Multi-Document Support**: Handle multiple PDFs simultaneously
- **Real-time Chat Interface**: Clean, responsive chat UI
- **Conversation History**: Track questions and answers per document

## ğŸ› ï¸ Tech Stack

### Backend
- **FastAPI** - Modern Python web framework
- **PostgreSQL/SQLite** - Document metadata storage
- **Google Gemini** - Large Language Model for question answering
- **FAISS** - Vector similarity search
- **LangChain** - LLM orchestration framework
- **PyMuPDF** - PDF text extraction
- **Sentence Transformers** - Text embeddings

### Frontend
- **React** with **TypeScript** - Modern web UI
- **Axios** - HTTP client for API communication
- **React Markdown** - Markdown rendering for AI responses
- **Lucide React** - Icon library
- **Custom CSS** - Responsive design

## ğŸ“ Project Structure

```
tree-pdf/
â”œâ”€â”€ backend/                    # FastAPI backend
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ core/              # Configuration and database
â”‚   â”‚   â”œâ”€â”€ documents/         # PDF upload and processing
â”‚   â”‚   â”œâ”€â”€ chat/              # Q&A functionality
â”‚   â”‚   â”œâ”€â”€ rag/               # RAG engine (embeddings, vector store)
â”‚   â”‚   â””â”€â”€ main.py            # FastAPI application
â”‚   â”œâ”€â”€ storage/               # File and index storage
â”‚   â”‚   â”œâ”€â”€ uploads/           # Uploaded PDF files
â”‚   â”‚   â””â”€â”€ indexes/           # FAISS vector indexes
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ frontend/                   # React frontend
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/        # React components
â”‚   â”‚   â”œâ”€â”€ services/          # API client
â”‚   â”‚   â”œâ”€â”€ types/             # TypeScript interfaces
â”‚   â”‚   â””â”€â”€ utils/             # Helper functions
â”‚   â””â”€â”€ package.json
â””â”€â”€ README.md
```

## ğŸš€ Quick Start

### Prerequisites
- Python 3.8+
- Node.js 16+
- Google Gemini API key

### Backend Setup

1. **Clone and navigate to backend**
   ```bash
   cd backend
   python -m venv venv
   source venv/bin/activate  # Windows: venv\Scripts\activate
   ```

2. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

3. **Environment configuration**
   ```bash
   cp .env.example .env
   # Edit .env and add your GEMINI_API_KEY
   ```

4. **Start backend server**
   ```bash
   cd app
   python main.py
   ```
   Backend runs on: `http://localhost:8000`

### Frontend Setup

1. **Navigate to frontend**
   ```bash
   cd frontend
   npm install
   ```

2. **Start development server**
   ```bash
   npm start
   ```
   Frontend runs on: `http://localhost:5173`

## ğŸ“– API Documentation

### Document Endpoints

#### Upload PDF
```http
POST /api/documents/upload
Content-Type: multipart/form-data

Body: file (PDF)
```

#### List Documents
```http
GET /api/documents/
```

#### Get Document
```http
GET /api/documents/{document_id}
```

### Chat Endpoints

#### Ask Question
```http
POST /api/chat/ask
Content-Type: application/json

{
  "document_id": 1,
  "question": "What is this document about?"
}
```

#### Get Conversation History
```http
GET /api/chat/history/{document_id}
```

## ğŸ—ï¸ Architecture Overview

### RAG Pipeline
1. **Document Processing**: PDF â†’ Text extraction â†’ Text chunking
2. **Indexing**: Text chunks â†’ Embeddings â†’ FAISS vector store
3. **Query Processing**: Question â†’ Embedding â†’ Similarity search
4. **Answer Generation**: Retrieved context + Question â†’ Gemini â†’ Answer

### Data Flow
```
User uploads PDF â†’ Backend processes â†’ Store in DB + Vector index
User asks question â†’ Search vectors â†’ Retrieve context â†’ Generate answer
```

## ğŸ”§ Configuration

### Environment Variables
```env
# Required
GEMINI_API_KEY=your_gemini_api_key

# Optional
DATABASE_URL=sqlite:///./storage/app.db
UPLOAD_MAX_SIZE=10485760
STORAGE_PATH=./storage
ENVIRONMENT=development
```

## ğŸ¯ Usage

1. **Start both backend and frontend servers**
2. **Open browser to frontend URL**
3. **Upload a PDF document** using the + button
4. **Wait for processing** (usually 10-30 seconds)
5. **Ask questions** about the document content
6. **Get AI-powered answers** based on the document

## ğŸ“Š Performance

- **Processing Speed**: ~2-5 seconds per page
- **Response Time**: ~3-5 seconds per question
- **Supported File Size**: Up to 10MB PDFs
- **Accuracy**: High context-aware and grounded responses using semantic search

## ğŸ”® Future Enhancements

- Document metadata extraction (title, summary, type)
- Multi-language support
- OCR for image-based PDFs
- Advanced conversation memory
- Cloud storage integration (AWS S3)
- User authentication and document sharing

## ğŸ› Troubleshooting

### Common Issues

**"Document not processing"**
- Check PDF is text-based (not scanned images)
- Verify Gemini API key is valid
- Check backend logs for errors

**"Vector index not found"**
- Ensure document processing completed successfully
- Check `storage/indexes/` directory exists

## ğŸ“„ License

This project is for educational purposes as part of a fullstack internship assignment.


---

## ğŸš€ Demo

[Live Demo Link] | [Demo Video Link]